{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 09:03:13.928062: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-15 09:03:15.057833: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-15 09:03:15.062576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-15 09:03:18,181 - DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
      "2023-09-15 09:03:18.559468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-15 09:03:18,719 - DEBUG - Creating converter from 7 to 5\n",
      "2023-09-15 09:03:18,722 - DEBUG - Creating converter from 5 to 7\n",
      "2023-09-15 09:03:18,726 - DEBUG - Creating converter from 7 to 5\n",
      "2023-09-15 09:03:18,727 - DEBUG - Creating converter from 5 to 7\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Tuple\n",
    "import pandas as pd \n",
    "import mlflow as mf\n",
    "\n",
    "## replace with your specific paths\n",
    "sys.path.append(\"/home/aamir07/mlops2_with_dagster/\") \n",
    "ARTIFACT_PATH = \"/home/aamir07/mlops2_with_dagster/artifacts/mlflow_artfacts\" \n",
    "LOGGER_FOLDER_PATH = \"/home/aamir07/mlops2_with_dagster/notebooks\"\n",
    "\n",
    "from mlops2_with_dagster import model_pipeline, features_pipeline, encoder_pipeline\n",
    "from pathlib import Path\n",
    "from joblib import dump, load\n",
    "from hamilton import driver, base\n",
    "from mlops2_with_dagster.utils import get_project_dir, printse\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    LabelEncoder\n",
    ")\n",
    "\n",
    "from ortho import BaseKernel\n",
    "from ortho.utils import Logger\n",
    "from ortho.ortho.decorators import task\n",
    "from ortho.ortho.callbacks import MlFlowCallBack, LoggerCallBack\n",
    "\n",
    "\n",
    "logger = Logger().logger()\n",
    "\n",
    "   \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>> project_dir: /home/aamir07/mlops2_with_dagster\n"
     ]
    }
   ],
   "source": [
    "project = 'mlops2_with_dagster'\n",
    "project_dir = get_project_dir(project)\n",
    "printse(f'project_dir: {project_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file: Path = project_dir/\"intermediate_data/encoder.joblib\"\n",
    "data: Path = project_dir/\"data/test.csv\"\n",
    "datatype: str = \"dataset\"\n",
    "model_file: Path = project_dir/\"warehouse/output.joblib\"\n",
    "infer_type: str = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aamir07/miniconda3/envs/test_with_poetry/lib/python3.10/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.1.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "df = pd.read_csv(data)\n",
    "encoders = load(encoder_file)\n",
    "clfinfo = load(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Infer(BaseKernel):\n",
    "    \n",
    "    def __init__(self, callbacks,  experiment_name, run_name, load_from_artifact=False):\n",
    "        super().__init__(callbacks,\n",
    "                       experiment_name,\n",
    "                       run_name,\n",
    "                       load_from_artifact)\n",
    "        \n",
    "        self.index_col = 'passengerid'\n",
    "        self.target_col = \"survived\"\n",
    "        self.cat_cols = [\"sex\", \"cabin\", \"embarked\"]\n",
    "        self.config = {\n",
    "            'index_column': self.index_col,\n",
    "            'target_column': self.target_col,\n",
    "            'categorical_columns': self.cat_cols\n",
    "        }\n",
    "        self.config_model = {\n",
    "            'index_column': self.index_col,\n",
    "            'target_column': self.target_col,\n",
    "            'random_state': 42,\n",
    "            'max_depth': None,\n",
    "            'validation_size_fraction': 0.33,\n",
    "            't': 0.5\n",
    "            }\n",
    "            \n",
    "        self.config_infer2 = {\n",
    "            'index_column': self.index_col,\n",
    "            'target_column': self.target_col,\n",
    "            't': 0.5\n",
    "            }\n",
    "\n",
    "    \n",
    "    @task(build_on_previous_run=True, end_mlflow_run=False)       \n",
    "    def inference(self, df_test=None, encoders=None, clfinfo=None):\n",
    "        infer2_adapter = base.SimplePythonGraphAdapter(base.DictResult())\n",
    "        infer2_dr = driver.Driver(self.config_infer2, \n",
    "                                    features_pipeline, model_pipeline, encoder_pipeline,\n",
    "                                    adapter = infer2_adapter)\n",
    "        dinfer2 = dict(\n",
    "            df = df_test,\n",
    "            clf = clfinfo['fit_clf'],\n",
    "            **encoders['encoders']\n",
    "        )\n",
    "        output_nodes = ['chain_predictions']\n",
    "        \n",
    "        output = infer2_dr.execute(output_nodes, inputs  = dinfer2)\n",
    "        \n",
    "        data = {\"inference_results\": output }\n",
    "        payload = {\"artifact_path\": ARTIFACT_PATH,\n",
    "                   \"params\": self.config_infer2}\n",
    "        \n",
    "        return data, payload\n",
    "    \n",
    "    def run(self, *args, **kwargs):\n",
    "        \n",
    "        return self.inference(*args, **kwargs)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 09:05:16,816 - DEBUG - Starting new HTTP connection (1): 127.0.0.1:5002\n",
      "2023-09-15 09:05:17,887 - DEBUG - http://127.0.0.1:5002 \"GET /api/2.0/mlflow/experiments/get-by-name?experiment_name=Mlflow_with_Dagster HTTP/1.1\" 200 241\n",
      "2023-09-15 09:05:17,903 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:19,738 - DEBUG - http://127.0.0.1:5002 \"GET /api/2.0/mlflow/runs/get?run_uuid=a05f805113ff42a3b54371547a26d49b&run_id=a05f805113ff42a3b54371547a26d49b HTTP/1.1\" 200 1572\n",
      "2023-09-15 09:05:19,748 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:20,971 - DEBUG - http://127.0.0.1:5002 \"POST /api/2.0/mlflow/runs/update HTTP/1.1\" 200 423\n",
      "2023-09-15 09:05:20,979 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:22,176 - DEBUG - http://127.0.0.1:5002 \"GET /api/2.0/mlflow/runs/get?run_uuid=a05f805113ff42a3b54371547a26d49b&run_id=a05f805113ff42a3b54371547a26d49b HTTP/1.1\" 200 1571\n",
      "2023-09-15 09:05:22,181 - INFO - Successfuly initiated run with name BadamBhum and id a05f805113ff42a3b54371547a26d49b\n",
      "2023-09-15 09:05:22,191 - WARNING - Note: Hamilton collects completely anonymous data about usage. This will help us improve Hamilton over time. See https://github.com/dagworks-inc/hamilton#usage-analytics--data-privacy for details.\n",
      "2023-09-15 09:05:22,198 - DEBUG - Computing clf.\n",
      "2023-09-15 09:05:22,201 - DEBUG - Computing df.\n",
      "2023-09-15 09:05:22,204 - DEBUG - Computing index_column.\n",
      "2023-09-15 09:05:22,208 - DEBUG - Computing input_data.\n",
      "2023-09-15 09:05:22,248 - DEBUG - Computing target_column.\n",
      "2023-09-15 09:05:22,251 - DEBUG - Computing features.\n",
      "2023-09-15 09:05:22,259 - DEBUG - Computing pclass.\n",
      "2023-09-15 09:05:22,264 - DEBUG - Computing age.\n",
      "2023-09-15 09:05:22,267 - DEBUG - Computing fare.\n",
      "2023-09-15 09:05:22,270 - DEBUG - Computing cabin.\n",
      "2023-09-15 09:05:22,275 - DEBUG - Computing cabin_t.\n",
      "2023-09-15 09:05:22,317 - DEBUG - Computing cabinencoder.\n",
      "2023-09-15 09:05:22,320 - DEBUG - Computing cabin_category.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 09:05:22,481 - DEBUG - Computing sex.\n",
      "2023-09-15 09:05:22,483 - DEBUG - Computing sexencoder.\n",
      "2023-09-15 09:05:22,485 - DEBUG - Computing sex_category.\n",
      "2023-09-15 09:05:22,512 - DEBUG - Computing embarked.\n",
      "2023-09-15 09:05:22,515 - DEBUG - Computing embarkedencoder.\n",
      "2023-09-15 09:05:22,517 - DEBUG - Computing embarked_category.\n",
      "2023-09-15 09:05:22,543 - DEBUG - Computing sibsp.\n",
      "2023-09-15 09:05:22,546 - DEBUG - Computing parch.\n",
      "2023-09-15 09:05:22,549 - DEBUG - Computing family.\n",
      "2023-09-15 09:05:22,554 - DEBUG - Computing engineered_features.\n",
      "2023-09-15 09:05:22,577 - DEBUG - Computing final_imputed_features.\n",
      "2023-09-15 09:05:22,583 - DEBUG - Computing t.\n",
      "2023-09-15 09:05:22,587 - DEBUG - Computing chain_predictions.\n",
      "2023-09-15 09:05:22,657 - DEBUG - Succeed in sending telemetry consisting of [b'{\"api_key\": \"phc_mZg8bkn3yvMxqvZKRlMlxjekFU5DFDdcdAsijJ2EH5e\", \"event\": \"os_hamilton_run_start\", \"properties\": {\"os_type\": \"posix\", \"os_version\": \"Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\", \"python_version\": \"3.10.11/CPython\", \"distinct_id\": \"f41b7308-274c-4333-ab85-eca6fe911802\", \"hamilton_version\": [1, 28, 0], \"telemetry_version\": \"0.0.1\", \"number_of_nodes\": 60, \"number_of_modules\": 3, \"number_of_config_items\": 3, \"decorators_used\": {\"parameterize_sources\": 4, \"extract_columns\": 2, \"extract_fields\": 1}, \"graph_adapter_used\": \"hamilton.base.SimplePythonGraphAdapter\", \"result_builder_used\": \"hamilton.base.DictResult\", \"driver_run_id\": \"798a181d-6446-4418-92c0-177afe227177\", \"error\": null, \"graph_executor_class\": \"DefaultGraphExecutor\"}}'].\n",
      "2023-09-15 09:05:28,756 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:28,827 - DEBUG - http://127.0.0.1:5002 \"POST /api/2.0/mlflow/runs/log-batch HTTP/1.1\" 200 2\n",
      "2023-09-15 09:05:28,839 - INFO - Successfuly stored inference_results to /home/aamir07/mlops2_with_dagster/artifacts/mlflow_artfacts/inference_results.pickle\n",
      "2023-09-15 09:05:28,848 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:28,942 - DEBUG - http://127.0.0.1:5002 \"GET /api/2.0/mlflow/runs/get?run_uuid=a05f805113ff42a3b54371547a26d49b&run_id=a05f805113ff42a3b54371547a26d49b HTTP/1.1\" 200 1571\n",
      "2023-09-15 09:05:28,980 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:29,070 - DEBUG - http://127.0.0.1:5002 \"PUT /api/2.0/mlflow-artifacts/artifacts/1/a05f805113ff42a3b54371547a26d49b/artifacts/inference_results.pickle HTTP/1.1\" 200 2\n",
      "2023-09-15 09:05:29,080 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:29,174 - DEBUG - http://127.0.0.1:5002 \"PUT /api/2.0/mlflow-artifacts/artifacts/1/a05f805113ff42a3b54371547a26d49b/artifacts/transformed_data.pickle HTTP/1.1\" 200 2\n",
      "2023-09-15 09:05:29,185 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:29,199 - DEBUG - http://127.0.0.1:5002 \"PUT /api/2.0/mlflow-artifacts/artifacts/1/a05f805113ff42a3b54371547a26d49b/artifacts/encoders.pickle HTTP/1.1\" 200 2\n",
      "2023-09-15 09:05:29,212 - DEBUG - Succeed in sending telemetry consisting of [b'{\"api_key\": \"phc_mZg8bkn3yvMxqvZKRlMlxjekFU5DFDdcdAsijJ2EH5e\", \"event\": \"os_hamilton_run_end\", \"properties\": {\"os_type\": \"posix\", \"os_version\": \"Linux-5.15.90.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\", \"python_version\": \"3.10.11/CPython\", \"distinct_id\": \"f41b7308-274c-4333-ab85-eca6fe911802\", \"hamilton_version\": [1, 28, 0], \"telemetry_version\": \"0.0.1\", \"is_success\": true, \"runtime_seconds\": 6.539501190185547, \"number_of_outputs\": 1, \"number_of_overrides\": 0, \"number_of_inputs\": 0, \"driver_run_id\": \"798a181d-6446-4418-92c0-177afe227177\", \"error\": null}}'].\n",
      "2023-09-15 09:05:29,227 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:35,122 - DEBUG - http://127.0.0.1:5002 \"PUT /api/2.0/mlflow-artifacts/artifacts/1/a05f805113ff42a3b54371547a26d49b/artifacts/training_outputs.pickle HTTP/1.1\" 200 2\n",
      "2023-09-15 09:05:35,191 - DEBUG - Resetting dropped connection: 127.0.0.1\n",
      "2023-09-15 09:05:35,249 - DEBUG - http://127.0.0.1:5002 \"PUT /api/2.0/mlflow-artifacts/artifacts/1/a05f805113ff42a3b54371547a26d49b/artifacts/target.pickle HTTP/1.1\" 200 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'chain_predictions': (array([0.82      , 0.15      , 0.2       , ..., 0.20566667, 0.13      ,\n",
       "         0.26      ]),\n",
       "  array([1, 0, 0, ..., 0, 0, 0]))}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dagstermill\n",
    "inferencer = Infer(experiment_name=\"Mlflow_with_Dagster\",\n",
    "                          run_name=\"BadamBhum\",\n",
    "                          callbacks = [LoggerCallBack(log_folder=LOGGER_FOLDER_PATH\n",
    "                                                      , kernel_names=[\"inference\"]),\n",
    "                                       MlFlowCallBack(kernel_names=[\"inference\"])],\n",
    "                          load_from_artifact=False)\n",
    "\n",
    "# mf.end_run() #Un-comment only when a run is ended prematurely or you run into any errors while running the function such that the current run can end!\n",
    "dagstermill.yield_result(inferencer.run(df_test=df, encoders=encoders, clfinfo=clfinfo)[0][\"inference_results\"], output_name=\"inference_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_with_poetry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
